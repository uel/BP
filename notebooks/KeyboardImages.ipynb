{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFfnbnMjSk5JxrBRuXEnZb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWZ_dFk6uOYa",
        "outputId": "e6708940-ef15-43f2-9e9b-cb72073a7088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BP'...\n",
            "remote: Enumerating objects: 1011, done.\u001b[K\n",
            "remote: Counting objects: 100% (1011/1011), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1003/1003), done.\u001b[K\n",
            "remote: Total 1011 (delta 8), reused 1011 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1011/1011), 21.59 MiB | 16.40 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "mv: 'BP' and './BP' are the same file\n",
            "renamed 'BP/data' -> './data'\n",
            "renamed 'BP/data.py' -> './data.py'\n",
            "renamed 'BP/keys.py' -> './keys.py'\n",
            "renamed 'BP/main.py' -> './main.py'\n",
            "renamed 'BP/__pycache__' -> './__pycache__'\n",
            "renamed 'BP/template' -> './template'\n",
            "renamed 'BP/tests.py' -> './tests.py'\n",
            "renamed 'BP/video_download.py' -> './video_download.py'\n",
            "rm: cannot remove 'BP': Is a directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/uel/BP\n",
        "!mv -v BP/* .\n",
        "!rmdir BP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from data import GetPointsFromXML\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tHw_D9rCyMeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, inverse_mappping=False):\n",
        "    # Resize image while maintaining the aspect ratio\n",
        "    target_height = 360\n",
        "    aspect_ratio = image.shape[1] / image.shape[0]\n",
        "    target_width = int(target_height * aspect_ratio)\n",
        "\n",
        "    if target_width > 640:\n",
        "        target_width = 640\n",
        "        target_height = int(target_width / aspect_ratio)\n",
        "\n",
        "    resized_image = cv2.resize(image, (target_width, target_height))\n",
        "    resize_ratio = target_width / image.shape[1]\n",
        "\n",
        "    # Place the resized image on a black background\n",
        "    background = np.zeros((360, 640, 3), dtype=np.uint8)\n",
        "    x_offset = (640 - target_width) // 2\n",
        "    y_offset = (360 - target_height) // 2\n",
        "    background[y_offset:y_offset+target_height, x_offset:x_offset+target_width] = resized_image\n",
        "\n",
        "    # Normalize pixel values between 0 and 1\n",
        "    normalized_image = background / 255.0\n",
        "\n",
        "    # Create function for coordinate mapping\n",
        "    if inverse_mappping:\n",
        "        map_coordinates = lambda x, y: ( (x*640 - x_offset)/resize_ratio, (y*360 - y_offset)/resize_ratio )\n",
        "    else:\n",
        "        map_coordinates = lambda x, y: ( (x*resize_ratio + x_offset) / 640, (y*resize_ratio + y_offset) / 360 )\n",
        "\n",
        "    return normalized_image, map_coordinates\n"
      ],
      "metadata": {
        "id": "oLV4oQ9Xzvcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadImages(dir, files=[], points=None):\n",
        "\n",
        "    if len(files) == 0:\n",
        "        files = os.listdir(dir)\n",
        "\n",
        "    points_res = []\n",
        "    images = []\n",
        "    for file in files:\n",
        "        img = cv2.imread(dir +\"/\"+ file)\n",
        "        normalized_image, map_coordinates = preprocess_image(img)\n",
        "        images.append(normalized_image)\n",
        "        if points is not None:\n",
        "            points_res.append(np.array([map_coordinates(x, y) for (x, y) in points[files.index(file)]]))\n",
        "\n",
        "    if points is not None:\n",
        "        return images, points_res\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "5epC3Cw-z26C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(image, bounding_box, n=5):\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "    bounding_box = bounding_box.reshape((4, 2))\n",
        "\n",
        "    # Convert bounding box coordinates to a list of tuples, scale them to the image size\n",
        "    bounding_box = [(x * 639, y * 359) for (x, y) in bounding_box]\n",
        "\n",
        "    transform = A.Compose([\n",
        "        A.RandomBrightnessContrast(p=1, brightness_limit=0.2, contrast_limit=0.2),\n",
        "        A.ShiftScaleRotate(p=1, border_mode=cv2.BORDER_CONSTANT, rotate_limit=3, scale_limit=0.1, shift_limit=0.1)\n",
        "    ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=True))\n",
        "\n",
        "\n",
        "    augmented_images = []\n",
        "    bounding_boxes = []\n",
        "\n",
        "    # Generate n augmentations\n",
        "    while len(augmented_images) < n:\n",
        "        # Apply the transformation to the image\n",
        "        augmented = transform(image=image, keypoints=bounding_box)\n",
        "        img = augmented['image']\n",
        "        b_box = augmented['keypoints']\n",
        "\n",
        "        # Append the augmented image and bounding box coordinates to the result\n",
        "        if len(b_box) == 4:\n",
        "            augmented_images.append(img/255.0)\n",
        "            b_box = np.array(b_box)\n",
        "            b_box[:, 0] /= 639\n",
        "            b_box[:, 1] /= 359\n",
        "            b_box = b_box.reshape((8,))\n",
        "            bounding_boxes.append(b_box)\n",
        "\n",
        "    return augmented_images, bounding_boxes"
      ],
      "metadata": {
        "id": "DruV94Pkz52G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = []"
      ],
      "metadata": {
        "id": "nsH7WrXv3TXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainDetectionModel():\n",
        "    detection = Sequential()\n",
        "    detection.add(Conv2D(32, (3, 3), activation='relu', input_shape=(360, 640, 3)))\n",
        "    detection.add(MaxPooling2D((2, 2)))\n",
        "    detection.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    detection.add(MaxPooling2D((2, 2)))\n",
        "    detection.add(Flatten())\n",
        "    detection.add(Dense(64, activation='relu'))\n",
        "    detection.add(Dense(1, activation='sigmoid'))\n",
        "    detection.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    with_keyboard = LoadImages(\"data/separated_frames/with_keyboard\")\n",
        "    without_keyboard = LoadImages(\"data/separated_frames/without_keyboard\")\n",
        "    min_class_count = min(len(with_keyboard), len(without_keyboard))\n",
        "    combined = np.array(with_keyboard[:min_class_count] + without_keyboard[:min_class_count])\n",
        "    labels = np.array([1.]*(min_class_count) + [0.]*(min_class_count))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(combined, labels, test_size=0.3, random_state=0)\n",
        "\n",
        "    detection.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test), batch_size=4)\n",
        "    detection.save(\"models/detection.h5\")\n",
        "    return detection"
      ],
      "metadata": {
        "id": "mdyHZ575z8dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainBoundingBoxModel(model_name, grayscale=False):\n",
        "    bounding_box = Sequential()\n",
        "\n",
        "    if grayscale:\n",
        "        bounding_box.add(Conv2D(32, (3, 3), activation='relu', input_shape=(360, 640, 1)))\n",
        "    else:\n",
        "        bounding_box.add(Conv2D(32, (3, 3), activation='relu', input_shape=(360, 640, 3)))\n",
        "\n",
        "    bounding_box.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    bounding_box.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    bounding_box.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    bounding_box.add(Flatten())\n",
        "    bounding_box.add(Dense(64, activation='relu'))\n",
        "    bounding_box.add(Dense(8, activation='sigmoid'))\n",
        "    bounding_box.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    try:\n",
        "        bounding_box = load_model(\"models/\" + model_name + \".h5\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    points, images = GetPointsFromXML(\"data/separated_frames/keyboard_annotations.xml\")\n",
        "    images, points = LoadImages(\"data/separated_frames/with_keyboard\", images, points)\n",
        "\n",
        "    if grayscale:\n",
        "        new_images = []\n",
        "        for image in images:\n",
        "            new_image = 0.2989 * image[:, :, 0] + 0.5870 * image[:, :, 1] + 0.1140 * image[:, :, 2]\n",
        "            new_images.append(new_image)\n",
        "        images = np.array(new_images)\n",
        "    else:\n",
        "        images = np.array(images)\n",
        "\n",
        "    points = np.array(points)\n",
        "    points = points.reshape((points.shape[0], 8))\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images, points, test_size=0.2, random_state=0)\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        # prediction = bounding_box.predict(np.array([test_img]))[0]\n",
        "        # prediction = prediction.reshape((4, 2))\n",
        "        aug_data, pts = augment_data(x_train[i], y_train[i], 4)\n",
        "        # for (x, y) in pts[0]:\n",
        "        #     cv2.circle(aug_data[0], (int(x*640), int(y*360)), 3, (0, 255, 0), -1)\n",
        "        # plt.imshow(aug_data[0])\n",
        "        # plt.show()\n",
        "        x_train = np.append(x_train, aug_data, axis=0)\n",
        "        y_train = np.append(y_train, pts, axis=0)\n",
        "\n",
        "    for i in range(len(x_test)):\n",
        "        aug_data, pts = augment_data(x_test[i], y_test[i], 4)\n",
        "        x_test = np.append(x_test, aug_data, axis=0)\n",
        "        y_test = np.append(y_test, pts, axis=0)\n",
        "\n",
        "    bounding_box.fit(x_train, y_train, epochs=32, validation_data=(x_test, y_test), batch_size=8)\n",
        "    bounding_box.save(\"models/\" + model_name + \".h5\")\n",
        "\n",
        "    return bounding_box"
      ],
      "metadata": {
        "id": "dEbHUsXhz-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainBoundingBoxModel(\"bounding_box\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-smCxqzI0BH5",
        "outputId": "31a27310-759d-4c51-aaeb-d9f2303b0611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "50/50 [==============================] - 5s 74ms/step - loss: 9.1805e-04 - val_loss: 9.9292e-04\n",
            "Epoch 2/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 0.0010 - val_loss: 9.5263e-04\n",
            "Epoch 3/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 5.4921e-04 - val_loss: 8.1954e-04\n",
            "Epoch 4/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 4.0512e-04 - val_loss: 7.9734e-04\n",
            "Epoch 5/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 2.6070e-04 - val_loss: 6.4599e-04\n",
            "Epoch 6/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6386e-04 - val_loss: 6.1766e-04\n",
            "Epoch 7/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.2344e-04 - val_loss: 6.1711e-04\n",
            "Epoch 8/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 1.1819e-04 - val_loss: 5.7817e-04\n",
            "Epoch 9/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 7.9496e-05 - val_loss: 5.9242e-04\n",
            "Epoch 10/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 6.3521e-05 - val_loss: 5.6755e-04\n",
            "Epoch 11/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 5.7144e-05 - val_loss: 5.5435e-04\n",
            "Epoch 12/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 5.6901e-05 - val_loss: 5.5780e-04\n",
            "Epoch 13/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 5.8707e-05 - val_loss: 5.6556e-04\n",
            "Epoch 14/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 7.0399e-05 - val_loss: 5.5332e-04\n",
            "Epoch 15/32\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 6.8860e-05 - val_loss: 5.5704e-04\n",
            "Epoch 16/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 7.9405e-05 - val_loss: 5.8317e-04\n",
            "Epoch 17/32\n",
            "50/50 [==============================] - 3s 57ms/step - loss: 8.4953e-05 - val_loss: 5.7946e-04\n",
            "Epoch 18/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 8.9178e-05 - val_loss: 5.5453e-04\n",
            "Epoch 19/32\n",
            "50/50 [==============================] - 3s 68ms/step - loss: 9.0028e-05 - val_loss: 6.4346e-04\n",
            "Epoch 20/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.1370e-04 - val_loss: 6.0417e-04\n",
            "Epoch 21/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.2276e-04 - val_loss: 6.2659e-04\n",
            "Epoch 22/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.4223e-04 - val_loss: 6.0639e-04\n",
            "Epoch 23/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.7637e-04 - val_loss: 6.3916e-04\n",
            "Epoch 24/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 1.4824e-04 - val_loss: 5.9605e-04\n",
            "Epoch 25/32\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 1.1929e-04 - val_loss: 5.8269e-04\n",
            "Epoch 26/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 8.3822e-05 - val_loss: 5.9406e-04\n",
            "Epoch 27/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 7.2664e-05 - val_loss: 6.0164e-04\n",
            "Epoch 28/32\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 5.9321e-05 - val_loss: 5.7016e-04\n",
            "Epoch 29/32\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 4.7700e-05 - val_loss: 5.9117e-04\n",
            "Epoch 30/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 4.1246e-05 - val_loss: 5.7281e-04\n",
            "Epoch 31/32\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 4.2073e-05 - val_loss: 5.6055e-04\n",
            "Epoch 32/32\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 3.8522e-05 - val_loss: 5.4933e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f8a033236a0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}